{
  "source": "pdf28.pdf",
  "documents": [
    {
      "text": "# Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability\n\n**BY ALEX JOHN LONDON**\n\n## Abstract\nAlthough decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access \u201cthe knowledge within the machine.\u201d Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. As William Swartout puts it, when a physician consults an expert, \u201c[t]he physician may question whether some factor was con...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "1",
        "page_num": "1",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# The Black Box of Deep Learning\n\n## Abstract\nThe abstract is not provided in the text.\n\n## Introduction\nThe introduction discusses the historical context of knowledge acquisition and the importance of causal understanding in various domains, particularly in the context of expertise and decision-making. It highlights the distinction between empirical knowledge and theoretical knowledge, using Aristotle's concepts of techne and empirics to illustrate the necessity of a theoretical framework in productive sciences.\n\n## Methods\nThe methods section is not explicitly provided in the text.\n\n## Results\nThe results section is not explicitly provided in the text.\n\n## Discussion\nThe discussion elaborates on the implications of deep learning systems, emphasizing their theory-agnostic nature. It explains how these systems learn from data without a pre-programmed understanding of causal relationships. The architecture of deep learning systems is described, including the layers of connected nodes t...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "2",
        "page_num": "2",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# The Opacity and Independence of Machine Learning in Medical Decision-Making\n\n## Abstract\nThe opacity, independence from an explicit domain model, and lack of causal insight associated with some powerful machine learning approaches are not radically different from routine aspects of medical decision-making. Despite this accuracy, deep learning systems can be black boxes. Although their designers understand the architecture of these systems and the process by which they generate the models they use for classification, the models themselves can be inscrutable to humans. Even when techniques are used to identify features or a set of features to which a model gives significant weight in evaluating a particular case, the relationships between those features and the output classification can be both indirect and fragile. A small permutation in a seemingly unrelated aspect of the data can result in a significantly different weighting of features. Moreover, different initial settings can res...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "3",
        "page_num": "3",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# Responsible Medical Decision-Making\n\n## Abstract\nThe practical findings from rigorous empirical testing are frequently more reliable and reflective of causal relationships than the theoretical claims that purport to ground and explain them. Medicine is a sphere where current theories of disease pathophysiology or drug mechanisms are often of unknown or uncertain value. Since animal and in vitro models are unreliable predictors of effects in humans, specific hypotheses generated by these theories are subjected to testing during the process of evaluating the interventions that they support and motivate.\n\n## Introduction\nAlthough the ambition of contemporary drug development is to leverage expanding knowledge about these factors to produce a more analytical and logical development process, roughly nine out of ten drugs that enter development are never approved for any indication\u2014and half of the drugs that enter phase III testing fail. Hidden within this summary statistic is the fact th...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "4",
        "page_num": "4",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# Abstract\nIn medicine, the ability to intervene effectively in the world by exploiting causal relationships often derives from experience and precedes clinicians\u2019 ability to understand why interventions work. Decision models\u2014such as biases stemming from the over- or underrepresentation of particular classes of individuals\u2014can be inherited by these systems. Without insight into how the models work, critics worry that the models may incorporate biases that are harmful enough to offset marginal gains in predictive power. In order to ward off such possibilities, critics hold that machine learning systems must at least be interpretable to humans.\n\n# Introduction\nIn a popular example, Rich Caruana and colleagues report that, although a neural net was more accurate than alternatives at diagnosing the probability of death from pneumonia, it ranked asthmatic patients as having a lower probability than the general population. This finding is \u201ccounterintuitive\u201d because patients with a history o...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "5",
        "page_num": "5",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# Interpretability and Accountability in Machine Learning Systems\n\n## Abstract\nThis paper discusses the interpretability of machine learning systems, particularly in the context of clinical applications. It argues that while interpretability is often desired, it may not necessarily lead to improved reliability or accuracy. The paper emphasizes the importance of empirical validation of machine learning systems and the need for accountability in their deployment.\n\n## Introduction\nInterpretability in machine learning is often equated with the ability to understand the decision-making process of a model. However, this paper posits that mere interpretability does not guarantee system reliability. The complexity of models, especially in deep learning, poses challenges to interpretability, and even simpler models can become opaque in complex scenarios. The paper highlights the need for a nuanced understanding of interpretability and its implications for clinical practice.\n\n## Methods\nThe pap...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "6",
        "page_num": "6",
        "image_map": "{}"
      }
    },
    {
      "text": "\n# The Strange Tale of Three Identical Strangers: Cinematic Lessons in Bioethics\n\n## Abstract\nTim Wardle\u2019s 2018 documentary film *Three Identical Strangers* is an exploration of identity, family, and loss. It\u2019s also about nature versus nurture and the boundaries of ethically permissible research, particularly research involving children. The film tells the story of identical triplets\u2014David Kellman, Bobby Shafran, and Eddy Galland\u2014who were separated soon after birth in 1961. A different family adopted each boy, without being told that their son had two identical brothers. Through sheer coincidence, at age nineteen, Bobby and Eddy met. After a local newspaper picked up their story and published a picture of them, David entered the fray. Their unlikely reunion became a national feel-good sensation. Then the real story began to unfold.\n\n## Introduction\nThe adoption agency responsible for finding the families was collaborating with a group of researchers working on a study about something. ...",
      "metadata": {
        "source": "pdf28.pdf",
        "file_path": "/Users/utkarsh/Desktop/RAG/Scientific_RAG_Prototype/data/pdf28.pdf",
        "doc_id": "pdf28",
        "page_label": "7",
        "page_num": "7",
        "image_map": "{}"
      }
    }
  ]
}