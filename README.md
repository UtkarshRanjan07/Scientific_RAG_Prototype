# Scientific RAG Prototype

A Retrieval-Augmented Generation (RAG) system for scientific PDF documents using LlamaIndex and ChromaDB.

## Features

- ğŸ“„ **PDF Extraction**: Uses LlamaParse for text, tables, equations, and figures
- ğŸ§© **Smart Chunking**: Preserves tables and equations as atomic units
- ğŸ”¢ **Multi-Modal Embedding**: Semantic enrichment for tables and equations
- ğŸ’¾ **ChromaDB Storage**: Persistent vector storage
- ğŸ” **Semantic Search**: Similarity-based retrieval with filtering
- ğŸ’¬ **Streamlit Chat**: Interactive Q&A interface with citations

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure API Keys

Create a `.env` file:

```bash
cp .env.example .env
```

Edit `.env` and add your OpenAI API key:

```
OPENAI_API_KEY=your_openai_api_key_here
```

### 3. Ingest Documents

Place your PDFs in the `data/` folder, then run:

```bash
python3 scripts/ingest.py
```

### 4. Run the Chat App

```bash
streamlit run app.py
```

## Project Structure

```
Scientific_RAG_Prototype/
â”œâ”€â”€ data/                    # Your PDF documents
â”œâ”€â”€ chroma_db/               # Vector database (auto-created)
â”œâ”€â”€ extracted/               # Cached extractions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extraction/          # PDF parsing
â”‚   â”œâ”€â”€ processing/          # Chunking & embedding
â”‚   â”œâ”€â”€ storage/             # ChromaDB management
â”‚   â”œâ”€â”€ retrieval/           # Search logic
â”‚   â””â”€â”€ chatbot/             # Chat engine
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.py            # Ingestion pipeline
â”œâ”€â”€ app.py                   # Streamlit UI
â”œâ”€â”€ config.py                # Configuration
â””â”€â”€ requirements.txt
```

## Architecture

```
PDF Documents â†’ LlamaParse â†’ Smart Chunking â†’ Embedding â†’ ChromaDB
                                                              â†“
User Query â†’ Semantic Search â†’ Re-ranking â†’ LLM â†’ Response + Citations
```

## Configuration

Edit `config.py` to customize:

- `CHUNK_SIZE`: Text chunk size (default: 512)
- `SIMILARITY_TOP_K`: Number of results to retrieve (default: 10)
- `LLM_MODEL`: GPT model for generation (default: gpt-4-turbo-preview)
